{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling using Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONCorpus:\n",
    "    \n",
    "    def __init__(self, dpath):\n",
    "        self.dpath = dpath\n",
    "        self.dictionary = Dictionary(self._gen_documents())\n",
    "        \n",
    "    def _gen_documents(self):\n",
    "        # An auxiliary generator\n",
    "        for fname in os.listdir(self.dpath):\n",
    "            with open(os.path.join(self.dpath, fname), 'r') as file:\n",
    "                tokenized_doc = json.load(file)   \n",
    "                yield tokenized_doc\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self._gen_documents():\n",
    "            yield doc\n",
    "            \n",
    "class BoWCorpus:\n",
    "    \n",
    "    def __init__(self, corpus, dictionary):\n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.corpus:\n",
    "            yield self.dictionary.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PREPROCESSED_DATA = 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = JSONCorpus(DIR_PREPROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 5\n",
    "MAX_DF_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orig = len(corpus.dictionary)\n",
    "corpus.dictionary.filter_extremes(no_below=MIN_DF, no_above=MAX_DF_RATIO, keep_n=None)\n",
    "\n",
    "print(f'Number of tokens before filtering: {num_orig}')\n",
    "print(f'Total number of filtered tokens: {num_orig - len(corpus.dictionary)}')\n",
    "print(f'Number of tokens after filtering: {len(corpus.dictionary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = BoWCorpus(corpus, corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_SMARTIRS = 'ltc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(corpus=bow_corpus, dictionary=corpus.dictionary, smartirs=TFIDF_SMARTIRS)\n",
    "tfidf_corpus = [tfidf_model[doc_bow] for doc_bow in tqdm(bow_corpus, total=bow_corpus.dictionary.num_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF model parameters\n",
    "NUM_PASSES = 5 \n",
    "CHUNK_SIZE = 2000\n",
    "RANDOM_STATE = 42\n",
    "LIST_NUM_TOPICS = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Coherence model parameters\n",
    "COH_METRIC = 'c_v'\n",
    "COH_NUM_PROCESSES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores = {}\n",
    "\n",
    "for num_topics in tqdm(LIST_NUM_TOPICS):\n",
    "    model = Nmf(corpus=tfidf_corpus,\n",
    "                id2word=corpus.dictionary, \n",
    "                num_topics=num_topics, \n",
    "                passes=NUM_PASSES, \n",
    "                chunksize=CHUNK_SIZE, \n",
    "                random_state=RANDOM_STATE)\n",
    "    coherence_model = CoherenceModel(model, texts=corpus, dictionary=corpus.dictionary, coherence=COH_METRIC, processes=COH_NUM_PROCESSES)\n",
    "    coherence_scores[num_topics] = coherence_model.get_coherence()\n",
    "    print(f'Num topics: {num_topics} | Coherence: {coherence_scores[num_topics]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = max(coherence_scores, key=coherence_scores.get)\n",
    "print(f'Recommended number of topics (based on Coherence score): {NUM_TOPICS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(coherence_scores.keys())\n",
    "y = list(coherence_scores.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, y)\n",
    "plt.title('Coherence score vs. number of topics')\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Coherence score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Nmf(corpus=tfidf_corpus,\n",
    "                  id2word=corpus.dictionary, \n",
    "                  num_topics=NUM_TOPICS, \n",
    "                  passes=NUM_PASSES, \n",
    "                  chunksize=CHUNK_SIZE, \n",
    "                  random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(doc_bow):\n",
    "    \"\"\"Returns a list of the most dominant topics based on topic probability, one per each document from the input corpus.\n",
    "    \"\"\"\n",
    "    return sorted(final_model.get_document_topics(doc_bow), key=lambda x: x[1], reverse=True)[0][0]\n",
    "\n",
    "def create_df(corpus, num_documents):\n",
    "    \"\"\"Creates an auxiliary data frame which holds data for further visualizations.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    word_counts = []\n",
    "    dominant_topics = []\n",
    "    \n",
    "    for doc in tqdm(corpus, total=num_documents):\n",
    "        doc_bow = corpus.dictionary.doc2bow(doc)\n",
    "        words.append(doc)\n",
    "        word_counts.append(len(doc))\n",
    "        try:\n",
    "            dominant_topics.append(get_dominant_topic(doc_bow))\n",
    "        except:\n",
    "            dominant_topics.append(0)\n",
    "        \n",
    "    return pd.DataFrame({'words': words, \n",
    "                         'word_count': word_counts, \n",
    "                         'dominant_topic': dominant_topics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documents = create_df(corpus=corpus, num_documents=corpus.dictionary.num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of document word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "sns.distplot(df_documents['word_count'])\n",
    "plt.title('Distribution of document word counts')\n",
    "plt.xlabel('Word count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of topic sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "sns.countplot(df_documents['dominant_topic'])\n",
    "plt.title('Number of documents per topic')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(df_documents['dominant_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_6_TOPICS = [x[0] for x in counter.most_common(6)]\n",
    "LC_6_TOPICS = [x[0] for x in counter.most_common()[-1:-7:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of word counts across 6 most frequent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 2\n",
    "ncol = 3\n",
    "\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(23, 10), sharex='row')\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        topic_id = MC_6_TOPICS[(i+1) * j]\n",
    "        ax[i, j].set_title(f'Topic: {topic_id}')\n",
    "        sns.distplot(df_documents.query(f'dominant_topic == {topic_id}')['word_count'], ax=ax[i, j])\n",
    "        ax[i, j].set_xlabel('Word count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of word counts across 6 least frequent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 2\n",
    "ncol = 3\n",
    "\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(23, 10), sharex='row')\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        topic_id = LC_6_TOPICS[(i+1) * j]\n",
    "        ax[i, j].set_title(f'Topic: {topic_id}')\n",
    "        sns.distplot(df_documents.query(f'dominant_topic == {topic_id}')['word_count'], ax=ax[i, j])\n",
    "        ax[i, j].set_xlabel('Word count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds for 6 most frequent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 2\n",
    "ncol = 3\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(25, 10), sharex='row')\n",
    "aux_id = 0\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        topic_id = MC_6_TOPICS[(aux_id)]\n",
    "        aux_id += 1\n",
    "        wordcloud = WordCloud(background_color='white', collocations=False, max_words=20).generate_from_frequencies(dict(final_model.show_topic(topic_id, topn=20)))\n",
    "        ax[i, j].set_title(f'Topic: {topic_id}')\n",
    "        ax[i, j].imshow(wordcloud)\n",
    "        ax[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds for 6 least frequent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 2\n",
    "ncol = 3\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(25, 10), sharex='row')\n",
    "aux_id = 0\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        topic_id = LC_6_TOPICS[(aux_id)]\n",
    "        aux_id += 1\n",
    "        wordcloud = WordCloud(background_color='white', collocations=False, max_words=20).generate_from_frequencies(dict(final_model.show_topic(topic_id, topn=20)))\n",
    "        ax[i, j].set_title(f'Topic: {topic_id}')\n",
    "        ax[i, j].imshow(wordcloud)\n",
    "        ax[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 topics\n",
    "\n",
    "nrow = 5\n",
    "ncol = 4\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(60, 40), sharex='row', constrained_layout=True)\n",
    "aux_id = 0\n",
    "\n",
    "topic_ids = range(20)\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        topic_id = topic_ids[(aux_id)]\n",
    "        aux_id += 1\n",
    "        wordcloud = WordCloud(background_color='white', collocations=False, max_words=20).generate_from_frequencies(dict(final_model.show_topic(topic_id, topn=20)))\n",
    "        ax[i, j].set_title(f'Topic: {topic_id}', fontsize=45)\n",
    "        ax[i, j].imshow(wordcloud)\n",
    "        ax[i, j].axis('off')\n",
    "plt.savefig(f'word_clouds_nmf_{NUM_TOPICS}.png') # Save into a file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
