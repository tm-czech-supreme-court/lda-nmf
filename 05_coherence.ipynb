{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary notebook for Topic Coherence calculation\n",
    "\n",
    "The notebook serves as an auxiliary tool for calculating so-called Coherence Score for both LDA and NMF models (pre-trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = 'coherence_scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TARGET_DIR):\n",
    "    os.makedirs(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONCorpus:\n",
    "    \n",
    "    def __init__(self, dpath):\n",
    "        self.dpath = dpath\n",
    "        self.dictionary = Dictionary(self._gen_documents())\n",
    "        \n",
    "    def _gen_documents(self):\n",
    "        # An auxiliary generator\n",
    "        for fname in os.listdir(self.dpath):\n",
    "            with open(os.path.join(self.dpath, fname), 'r') as file:\n",
    "                tokenized_doc = json.load(file)   \n",
    "                yield tokenized_doc\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self._gen_documents():\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PREPROCESSED_DATA = 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = JSONCorpus(DIR_PREPROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 5\n",
    "MAX_DF_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orig = len(corpus.dictionary)\n",
    "corpus.dictionary.filter_extremes(no_below=MIN_DF, no_above=MAX_DF_RATIO, keep_n=None)\n",
    "\n",
    "print(f'Number of tokens before filtering: {num_orig}')\n",
    "print(f'Total number of filtered tokens: {num_orig - len(corpus.dictionary)}')\n",
    "print(f'Number of tokens after filtering: {len(corpus.dictionary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Coherence calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.nmf import Nmf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the pre-trained models? :-)\n",
    "DIR_LDA_MODELS = os.path.join('models', 'gensim', 'lda')\n",
    "DIR_NMF_MODELS = os.path.join('models', 'gensim', 'nmf')\n",
    "\n",
    "# Coherence model parameters\n",
    "COH_METRIC = 'c_v'\n",
    "COH_NUM_PROCESSES = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_coherence_scores = []\n",
    "\n",
    "for subdir in tqdm(os.listdir(DIR_LDA_MODELS)):\n",
    "    lda_model = LdaModel.load(os.path.join(DIR_LDA_MODELS, subdir, f'{subdir}.model'))\n",
    "    coherence_model = CoherenceModel(lda_model, texts=corpus, dictionary=corpus.dictionary, coherence=COH_METRIC, processes=COH_NUM_PROCESSES)\n",
    "    lda_coherence_scores.append((lda_model.num_topics, coherence_model.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_res = pd.DataFrame(sorted(lda_coherence_scores), columns=['num_topics', 'coherence_score_cv'])\n",
    "df_lda_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Coherence vs. number of topics (LDA)')\n",
    "plt.plot(df_lda_res['num_topics'], df_lda_res['coherence_score_cv'])\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Coherence score (CV)')\n",
    "plt.xticks(ticks=range(0, 101, 10))\n",
    "plt.savefig(os.path.join(TARGET_DIR, 'lda_coherence_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_res.to_csv(os.path.join(os.path.join(TARGET_DIR, 'lda_coherence_scores.csv')), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_coherence_scores = []\n",
    "\n",
    "for subdir in tqdm(os.listdir(DIR_NMF_MODELS)):\n",
    "    nmf_model = Nmf.load(os.path.join(DIR_NMF_MODELS, subdir, f'{subdir}.model'))\n",
    "    coherence_model = CoherenceModel(nmf_model, texts=corpus, dictionary=corpus.dictionary, coherence=COH_METRIC, processes=COH_NUM_PROCESSES)\n",
    "    nmf_coherence_scores.append((nmf_model.num_topics, coherence_model.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_res = pd.DataFrame(sorted(nmf_coherence_scores), columns=['num_topics', 'coherence_score_cv'])\n",
    "df_nmf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Coherence vs. number of topics (NMF)')\n",
    "plt.plot(df_nmf_res['num_topics'], df_nmf_res['coherence_score_cv'])\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Coherence score (CV)')\n",
    "plt.xticks(ticks=range(0, 101, 10))\n",
    "plt.savefig(os.path.join(TARGET_DIR, 'nmf_coherence_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_res.to_csv(os.path.join(os.path.join(TARGET_DIR, 'nmf_coherence_scores.csv')), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
