{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary notebook for training multiple topic models (LDA, NMF)\n",
    "The notebook serves as an auxiliary tool for training both LDA and NMF models.  \n",
    "User can specify a list of number of topics. The result is a set of models for each of the specified topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model, fpath):\n",
    "    dpath, fname = os.path.split(fpath)\n",
    "\n",
    "    if not os.path.exists(dpath):\n",
    "        os.makedirs(dpath)\n",
    "    model.save(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONCorpus:\n",
    "    \n",
    "    def __init__(self, dpath):\n",
    "        self.dpath = dpath\n",
    "        self.dictionary = Dictionary(self._gen_documents())\n",
    "        \n",
    "    def _gen_documents(self):\n",
    "        # An auxiliary generator\n",
    "        for fname in os.listdir(self.dpath):\n",
    "            with open(os.path.join(self.dpath, fname), 'r') as file:\n",
    "                tokenized_doc = json.load(file)   \n",
    "                yield tokenized_doc\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self._gen_documents():\n",
    "            yield doc\n",
    "            \n",
    "class BoWCorpus:\n",
    "    \n",
    "    def __init__(self, corpus, dictionary):\n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.corpus:\n",
    "            yield self.dictionary.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PREPROCESSED_DATA = 'preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = JSONCorpus(DIR_PREPROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 5\n",
    "MAX_DF_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orig = len(corpus.dictionary)\n",
    "corpus.dictionary.filter_extremes(no_below=MIN_DF, no_above=MAX_DF_RATIO, keep_n=None)\n",
    "\n",
    "print(f'Number of tokens before filtering: {num_orig}')\n",
    "print(f'Total number of filtered tokens: {num_orig - len(corpus.dictionary)}')\n",
    "print(f'Number of tokens after filtering: {len(corpus.dictionary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = BoWCorpus(corpus, corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_SMARTIRS = 'ltc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(corpus=bow_corpus, dictionary=bow_corpus.dictionary, smartirs=TFIDF_SMARTIRS)\n",
    "tfidf_corpus = [tfidf_model[doc_bow] for doc_bow in tqdm(bow_corpus, total=bow_corpus.dictionary.num_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_model(tfidf_model, os.path.join('models', 'gensim', 'tfidf', 'tfidf.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_pyldavis(lda_model, bow_corpus, fpath):\n",
    "    # Create an in-memory corpus\n",
    "    inmemory_bow_corpus = list(bow_corpus)\n",
    "    \n",
    "    # pyLDAvis data preparation\n",
    "    lda_vis_data = pyLDAvis.gensim.prepare(lda_model, inmemory_bow_corpus, bow_corpus.dictionary)\n",
    "    \n",
    "    # Output: HTML\n",
    "    dpath, fname = os.path.split(fpath)\n",
    "    if not os.path.exists(dpath):\n",
    "        os.makedirs(dpath)\n",
    "    pyLDAvis.save_html(lda_vis_data, fpath)\n",
    "\n",
    "def create_nmf_pyldavis(nmf_model, tfidf_corpus, dictionary, fpath):\n",
    "    topic_term_dists = nmf_model.get_topics()\n",
    "    vocab = np.array([dictionary.id2token[i] for i in range(len(dictionary))])\n",
    "    term_frequency = np.array([dictionary.cfs[i] for i in range(len(dictionary))])\n",
    "    doc_topic_dists = np.zeros(shape=(dictionary.num_docs, nmf_model.num_topics))\n",
    "    doc_lengths = np.zeros(shape=(dictionary.num_docs,))\n",
    "    \n",
    "    for i, doc_tfidf in enumerate(tfidf_corpus):\n",
    "        topic_dist = nmf_model.get_document_topics(doc_tfidf)\n",
    "        for x in topic_dist:\n",
    "            doc_topic_dists[i, x[0]] = x[1]\n",
    "        doc_lengths[i] = len(doc_tfidf)\n",
    "        \n",
    "    # Normalization: row sum must be equal to one\n",
    "    topic_term_dists = topic_term_dists / topic_term_dists.sum(axis=1)[:, None]\n",
    "    doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)[:, None]\n",
    "    \n",
    "    # Empty document filtering\n",
    "    mask = (doc_lengths != 0)\n",
    "    doc_topic_dists = doc_topic_dists[mask]\n",
    "    doc_lengths = doc_lengths[mask]\n",
    "    \n",
    "    # pyLDAvis data preparation\n",
    "    nmf_vis_data = pyLDAvis.prepare(topic_term_dists=topic_term_dists, \n",
    "                                    doc_topic_dists=doc_topic_dists, \n",
    "                                    doc_lengths=doc_lengths, \n",
    "                                    vocab=vocab, \n",
    "                                    term_frequency=term_frequency)\n",
    "    \n",
    "    # Output: HTML\n",
    "    dpath, fname = os.path.split(fpath)\n",
    "    if not os.path.exists(dpath):\n",
    "        os.makedirs(dpath)\n",
    "    pyLDAvis.save_html(nmf_vis_data, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 1 # Online learning\n",
    "NUM_PASSES = 5 # Sufficient - selected during convergence monitoring\n",
    "NUM_ITERATIONS = 200 # Sufficient - selected during convergence monitoring\n",
    "CHUNK_SIZE = 2000\n",
    "RANDOM_STATE = 42\n",
    "LIST_NUM_TOPICS = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in tqdm(LIST_NUM_TOPICS):\n",
    "    # LDA model\n",
    "    lda_model = LdaModel(corpus=bow_corpus,\n",
    "                         id2word=bow_corpus.dictionary, \n",
    "                         num_topics=num_topics, \n",
    "                         passes=NUM_PASSES, \n",
    "                         iterations=NUM_ITERATIONS,\n",
    "                         chunksize=CHUNK_SIZE, \n",
    "                         random_state=RANDOM_STATE, \n",
    "                         update_every=UPDATE_EVERY)\n",
    "    serialize_model(model=lda_model, fpath=os.path.join('models', 'gensim', 'lda', f'lda_{num_topics}', f'lda_{num_topics}.model'))\n",
    "    create_lda_pyldavis(lda_model, bow_corpus, fpath=os.path.join('pyldavis', 'lda', f'lda_{num_topics}.html'))\n",
    "    \n",
    "    # NMF model\n",
    "    nmf_model = Nmf(corpus=tfidf_corpus,\n",
    "                    id2word=bow_corpus.dictionary, \n",
    "                    num_topics=num_topics, \n",
    "                    passes=NUM_PASSES, \n",
    "                    chunksize=CHUNK_SIZE, \n",
    "                    random_state=RANDOM_STATE)\n",
    "    serialize_model(model=nmf_model, fpath=os.path.join('models', 'gensim', 'nmf', f'nmf_{num_topics}', f'nmf_{num_topics}.model'))\n",
    "    create_nmf_pyldavis(nmf_model=nmf_model, tfidf_corpus=tfidf_corpus, dictionary=bow_corpus.dictionary, fpath=os.path.join('pyldavis', 'nmf', f'nmf_{num_topics}.html'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
