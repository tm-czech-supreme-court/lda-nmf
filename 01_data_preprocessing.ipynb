{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition  \n",
    "The main data source is here <https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-3052>.  \n",
    "\n",
    "Data acquisition process is fully automated through the following steps:  \n",
    "* ZIP file which contains the whole dataset is downloaded from the main data source into a directory specified by **DIR_RAW_DATA** constant.\n",
    "* All documents are extracted from the ZIP file into a separate directory, specified by **DIR_EXTRACTED_DATA** constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RAW_DATA = 'https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11372/LRT-3052/SupCo.zip?sequence=3&isAllowed=y'\n",
    "FNAME_ZIP_DATA = 'SupCo.zip'\n",
    "DIR_RAW_DATA = 'raw_data'\n",
    "DIR_EXTRACTED_DATA = 'extracted_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(url, local_fpath):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(local_fpath, 'wb') as file, tqdm(desc=local_fpath, total=total, unit='iB', unit_scale=True, unit_divisor=1024,) as bar:\n",
    "        for data in resp.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR_RAW_DATA):\n",
    "    os.mkdir(DIR_RAW_DATA)\n",
    "    download_dataset(URL_RAW_DATA, os.path.join(DIR_RAW_DATA, FNAME_ZIP_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR_EXTRACTED_DATA):\n",
    "    with zipfile.ZipFile(os.path.join(DIR_RAW_DATA, FNAME_ZIP_DATA)) as zip_file:\n",
    "        for zip_info in tqdm(zip_file.infolist()):\n",
    "            fname = os.path.basename(zip_info.filename)\n",
    "            if fname.endswith('.txt'):\n",
    "                zip_info.filename = fname\n",
    "                zip_file.extract(zip_info, DIR_EXTRACTED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of extracted documents: {len(os.listdir(DIR_EXTRACTED_DATA))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "The following section describes a process of data-preprocessing aswell as a parallel processing approach to such task.\n",
    "\n",
    "### Input format:\n",
    "Raw text files (encoding: Windows-1250).\n",
    "\n",
    "### Pre-processing steps:\n",
    "#### I. Text cleaning\n",
    "* Numbers are separated from strings, eg. 'abc123' -> 'abc 123'.\n",
    "* Quotation marks are removed.\n",
    "* Leading and trailing spaces are removed.\n",
    "\n",
    "#### II. Document filtering:\n",
    "* Documents containing the following terms: **důvodnění**, **d ů v o d n ě n í** are selected.\n",
    "\n",
    "#### III. Part-of-speech tagging (POS tagging)\n",
    "* Filtering of tokens based on POS tags: nouns and adjectives are selected.\n",
    "\n",
    "#### IV. Lemmatization\n",
    "* The filtered tokens with length >= 3 are lemmatized and converted to lowercase.\n",
    "\n",
    "### Output format:\n",
    "JSON files (encoding: UTF-8) containing a list of extracted lemmas (one file per each document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_udpipe\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "    token = ' '.join(re.split('(\\d+)', token)) # Separate numbers\n",
    "    token = re.sub(r'[„“\\\"\\']+', '', token) # Remove quotation marks\n",
    "    token = token.strip() # Strip leading and trailing spaces\n",
    "    return token\n",
    "\n",
    "def extract_lemmas(text, spacy_model, allowed_pos_tags, min_token_len, to_lower):\n",
    "    lemmas = []\n",
    "    doc = spacy_model(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and token.pos_ in allowed_pos_tags and len(token) >= min_token_len:\n",
    "            if to_lower:\n",
    "                lemma = token.lemma_.lower()\n",
    "            else:\n",
    "                lemma = token.lemma_\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "    \n",
    "def preprocess_file(fpath, spacy_model, allowed_pos_tags, min_token_len, lemmas_to_lower, encoding):\n",
    "    with open(fpath, 'r', encoding=encoding, errors='ignore') as file:\n",
    "        textorig = ' '.join([preprocess_token(token) for token in file.read().split(' ')])\n",
    "        \n",
    "        if 'důvodnění' in textorig:\n",
    "            index = textorig.find('důvodnění')\n",
    "            text = textorig[index+9:]\n",
    "            return extract_lemmas(text, spacy_model=spacy_model, allowed_pos_tags=allowed_pos_tags, min_token_len=min_token_len, to_lower=lemmas_to_lower)\n",
    "            \n",
    "        elif 'd ů v o d n ě n í' in textorig:\n",
    "            index = textorig.find('d ů v o d n ě n í')\n",
    "            text = textorig[index+16:]\n",
    "            return extract_lemmas(text, spacy_model=spacy_model, allowed_pos_tags=allowed_pos_tags, min_token_len=min_token_len, to_lower=lemmas_to_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel pre-processing\n",
    "Multiprocessing can be applied in order to speed-up the pre-processing of data.  \n",
    "The function **worker_func** is fully parametrized via constant variables as it serves only as an auxiliary function for multiprocessing.\n",
    "\n",
    "**Parameters**:\n",
    "* **INPUT_ENCODING**: Encoding of the extracted (input) documents (Default: 'Windows-1250).\n",
    "* **OUTPUT_ENCODING**: Encoding of the output JSON files. (Default: 'utf-8')\n",
    "* **DIR_PREPROCESSED_DATA**: Target directory for the pre-processed JSON files.\n",
    "* **N_PROCESSES**: Number of parallel Python processes to execute.\n",
    "* **ALLOWED_POS_TAGS**: Part-of-speech tags used for token filtering -> a token is selected if it's POS tag is specified here. (Default: {'NOUN','ADJ'})\n",
    "* **MIN_TOKEN_LEN**: Minimal token length before lemmatization. (Default: 3)\n",
    "* **LEMMAS_TO_LOWERCASE**: If True, convert the lemmas to lowercase. (Default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Maximum recommended number of parallel processes: {mp.cpu_count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ENCODING = 'Windows-1250'\n",
    "OUTPUT_ENCODING = 'utf-8'\n",
    "DIR_PREPROCESSED_DATA = 'preprocessed_data'\n",
    "N_PROCESSES = 14\n",
    "\n",
    "ALLOWED_POS_TAGS = {'NOUN','ADJ'}\n",
    "MIN_TOKEN_LEN = 3 # <3, ...>\n",
    "LEMMAS_TO_LOWERCASE = True\n",
    "\n",
    "spacy_udpipe.download('cs')\n",
    "MODEL = spacy_udpipe.load('cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of documents to process: {len(os.listdir(DIR_EXTRACTED_DATA))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_func(fname):\n",
    "    input_fpath = os.path.join(DIR_EXTRACTED_DATA, fname)\n",
    "    output_fpath = os.path.join(DIR_PREPROCESSED_DATA, f'{fname.split(\".\")[0]}.json')\n",
    "\n",
    "    preprocessed_data = preprocess_file(input_fpath, \n",
    "                                        spacy_model=MODEL, # Thread safe\n",
    "                                        allowed_pos_tags=ALLOWED_POS_TAGS, \n",
    "                                        min_token_len=MIN_TOKEN_LEN,\n",
    "                                        lemmas_to_lower=LEMMAS_TO_LOWERCASE,\n",
    "                                        encoding=INPUT_ENCODING)\n",
    "    \n",
    "    # Check if we have successfully extracted any lemmas and create an output JSON file\n",
    "    if preprocessed_data is not None:\n",
    "        with open(output_fpath, 'w', encoding=OUTPUT_ENCODING) as output_file:\n",
    "            json.dump(obj=preprocessed_data, fp=output_file, ensure_ascii=False)\n",
    "\n",
    "if not os.path.exists(DIR_PREPROCESSED_DATA):\n",
    "    os.mkdir(DIR_PREPROCESSED_DATA)\n",
    "\n",
    "input_fnames = os.listdir(DIR_EXTRACTED_DATA)\n",
    "\n",
    "with mp.Pool(processes=N_PROCESSES) as pool:\n",
    "    for _ in tqdm(pool.imap_unordered(worker_func, input_fnames), total=len(input_fnames)):\n",
    "        # Progress bar hack :-)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of pre-processed documents: {len(os.listdir(DIR_PREPROCESSED_DATA))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = []\n",
    "\n",
    "for fname in tqdm(os.listdir('preprocessed_data')):\n",
    "    with open(os.path.join('preprocessed_data', fname), 'r') as f:\n",
    "        res = json.load(f)\n",
    "        doc_lengths.append(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Distribution of number of tokens per each lemmatized document')\n",
    "plt.xlabel('Number of tokens')\n",
    "sns.distplot(doc_lengths)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
